{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Torah Analyzer (ver. 3.0)\n",
        "\n",
        "***\n",
        "\n",
        "Powered by tegridy-tools: https://github.com/asigalov61/tegridy-tools\n",
        "\n",
        "***\n",
        "\n",
        "WARNING: This complete implementation is a functioning model of the Artificial Intelligence. Please excercise great humility, care, and respect. https://www.nscai.gov/\n",
        "\n",
        "***\n",
        "\n",
        "#### Project Los Angeles\n",
        "\n",
        "#### Tegridy Code 2024\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "cHXBchjTTcS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title NVIDA GPU Check\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "RAz7vARfQRO5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLU97viQMnXi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Install requirements\n",
        "!git clone https://github.com/asigalov61/tegridy-tools\n",
        "!pip install transformers -U\n",
        "!pip install accelerate -U\n",
        "!pip install gradio -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Import needed modules\n",
        "\n",
        "print('=' * 70)\n",
        "print('Loading modules...')\n",
        "print('=' * 70)\n",
        "\n",
        "%cd /content/tegridy-tools/tegridy-tools/\n",
        "\n",
        "import TORAH\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "print('=' * 70)\n",
        "print('Done!')\n",
        "print('=' * 70)"
      ],
      "metadata": {
        "id": "UFjK5rTjYvT8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load/Reload Mistral 7B Instruct model\n",
        "\n",
        "#@markdown https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map='cuda')"
      ],
      "metadata": {
        "id": "HLM3MAMgMv_O",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Start/Restart Torah Analyzer Gradio interface\n",
        "\n",
        "#===============================================================================\n",
        "# Model settings\n",
        "\n",
        "gen_len = 5000\n",
        "num_batches = 1\n",
        "temperature = 0.9\n",
        "\n",
        "#===============================================================================\n",
        "\n",
        "questions = [\n",
        "            'None',\n",
        "            'Please summarize the following text. Thank you.',\n",
        "            'Please write fifty questions about the provided text. Thank you.',\n",
        "            'Please write ten summary questions in order of importance. Thank you.',\n",
        "            'Please write twenty questions about the text in order of significance. Thank you.',\n",
        "            'Please write a short comprehensive essay about the most significant and important points of the text. Thank you.',\n",
        "            'Please write a short comprehensive literary summary essay about the most significant and important points of the text. Thank you.',\n",
        "            'Please write a short fictional essay based on the text. Thank you.',\n",
        "            'Please write a very long fictional essay based on the text. Thank you.',\n",
        "            'Please write a rephrased version of the provided text. Thank you.',\n",
        "            'Please write ten literary fictional titles for the provided text. Thank you.',\n",
        "            'Please write all possible questions about the provided text. Thank you.',\n",
        "            'Please write two hundred questions about the provided text. Thank you.',\n",
        "            'Please write ten most subtle, elusive, and tenious details from the provided text. Thank you.',\n",
        "            'Please write detailed statistical and numerical analysis of the provided text. Thank you.',\n",
        "            'Please write a single sentence summary of the provided text. Thank you.',\n",
        "            'Please write ten keywords of the text in order of importance and significance. Thank you.',\n",
        "            'What is the most important point of the text? Thank you.',\n",
        "            'What are the key numbers in the text? Thank you.',\n",
        "            'What are the key points of the text? Thank you.',\n",
        "            'What are the keywords of the text? Thank you.',\n",
        "            'What is the most significant point of the text?. Thank you.',\n",
        "            'What is the most intricate and delicate point in the provided text? Thank you.',\n",
        "\n",
        "            ]\n",
        "\n",
        "#===============================================================================\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    def fetch_torah_portion(torah_portion):\n",
        "      if torah_portion != 'None':\n",
        "        tp = TORAH.torah_weekly_portion_getter(torah_portion_name=torah_portion)[2]\n",
        "        return tp\n",
        "\n",
        "      else:\n",
        "        return None\n",
        "\n",
        "    def fetch_torah_chapter(torah_chapter):\n",
        "      if torah_chapter != 'None':\n",
        "        book, chapter = torah_chapter.split(' ')\n",
        "        tp = TORAH.torah_books_chapter_getter(torah_book_name=book, torah_book_chapter_number=int(chapter))[2]\n",
        "        return tp\n",
        "\n",
        "      else:\n",
        "        return None\n",
        "\n",
        "    torah_portions = ['None'] + [t[0] for t in TORAH.TORAH_WEEKLY_PORTIONS]\n",
        "\n",
        "    chaps_vers = sorted(set(tuple(cv) for cv in [(int(t[0]), int(t[1])) for t in TORAH.TORAH_TEXT_CLV]))\n",
        "    torah_chapters = ['None'] + [TORAH.TORAH_BOOKS[int(cv[0])-1] + ' ' + str(cv[1]) for cv in chaps_vers]\n",
        "\n",
        "    gr.Markdown(\n",
        "    \"\"\"\n",
        "    # Torah Analyser\n",
        "    ## Select Torah portion or chapter to analyze\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "    prompts = gr.Dropdown(torah_portions, label=\"Select desired Torah weekly portion to analyze\", value='None')\n",
        "    prompts1 = gr.Dropdown(torah_chapters, label=\"Select desired Torah book chapter to analyze\", value='None')\n",
        "    imsg = gr.Textbox(label='Torah text that will be analyzed')\n",
        "\n",
        "    gr.Markdown(\n",
        "    \"\"\"\n",
        "    ## Analysis output\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "    chatbot = gr.Chatbot(label='Torah Analyser', show_copy_button=True)\n",
        "    clear = gr.ClearButton([chatbot])\n",
        "\n",
        "    gr.Markdown(\n",
        "    \"\"\"\n",
        "    ## Manual Analysis\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "    cmsg = gr.Dropdown(questions, label=\"Select a sample question to ask\")\n",
        "    msg = gr.Textbox(label='Enter your own question about the text')\n",
        "\n",
        "    gr.Markdown(\n",
        "    \"\"\"\n",
        "    ## Automatic Analysis\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "    aa_btn = gr.Button(value='Auto-Analysis')\n",
        "\n",
        "    def respond(message, chat_history, imessage):\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        instruct_prompt = message\n",
        "\n",
        "        #========================================================================\n",
        "\n",
        "        if imessage and message != 'None':\n",
        "\n",
        "          prompt = '<s>[INST] ' + instruct_prompt + '\\n' + imessage + '\\n' + instruct_prompt + ' [/INST]' + '\\n'\n",
        "\n",
        "          input_ids = torch.LongTensor([tokenizer.encode(prompt)] * num_batches).cuda()\n",
        "\n",
        "          output = model.generate(input_ids,\n",
        "                                  max_length=len(input_ids[0]) + gen_len,\n",
        "                                  temperature=temperature,\n",
        "                                  do_sample=True,\n",
        "                                  pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "          generated_ids = output[:, len(input_ids[0]):]\n",
        "\n",
        "          for g in generated_ids:\n",
        "            generated_text = tokenizer.decode(g, skip_special_tokens=True)\n",
        "\n",
        "          bot_message = generated_text\n",
        "\n",
        "          chat_history.append((message, bot_message))\n",
        "\n",
        "          return \"\", chat_history\n",
        "\n",
        "        else:\n",
        "          bot_message = 'No Torah portion was selected to analyze!\\nPlease select Torah portion first!'\n",
        "\n",
        "          chat_history.append((message, bot_message))\n",
        "\n",
        "          return \"\", chat_history\n",
        "\n",
        "    def auto_analysis(chat_history, imessage):\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        message = 'Auto-Analysis:'\n",
        "\n",
        "        instruct_prompt = 'Please write thirty textual analysis prompts for an arbitrary text. Thank you.'\n",
        "\n",
        "        #=======================================================================\n",
        "\n",
        "        if imessage:\n",
        "\n",
        "          #=====================================================================\n",
        "\n",
        "          prompt = '<s>[INST] ' + instruct_prompt + '\\n' + imessage + '\\n' + instruct_prompt + ' [/INST]' + '\\n'\n",
        "\n",
        "          input_ids = torch.LongTensor([tokenizer.encode(prompt)] * num_batches).cuda()\n",
        "\n",
        "          output = model.generate(input_ids,\n",
        "                                  max_length=len(input_ids[0]) + gen_len,\n",
        "                                  temperature=temperature,\n",
        "                                  do_sample=True,\n",
        "                                  pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "          generated_ids = output[:, len(input_ids[0]):]\n",
        "\n",
        "          for g in generated_ids:\n",
        "            generated_text = tokenizer.decode(g, skip_special_tokens=True)\n",
        "\n",
        "          #=====================================================================\n",
        "\n",
        "          instruct_prompt = generated_text\n",
        "\n",
        "          prompt = '<s>[INST] ' + imessage + '\\n' + instruct_prompt + ' [/INST]' + '\\n'\n",
        "\n",
        "          input_ids = torch.LongTensor([tokenizer.encode(prompt)] * num_batches).cuda()\n",
        "\n",
        "          output = model.generate(input_ids,\n",
        "                                  max_length=len(input_ids[0]) + gen_len,\n",
        "                                  temperature=temperature,\n",
        "                                  do_sample=True,\n",
        "                                  pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "          generated_ids = output[:, len(input_ids[0]):]\n",
        "\n",
        "          for g in generated_ids:\n",
        "            generated_text = tokenizer.decode(g, skip_special_tokens=True)\n",
        "\n",
        "          bot_message = generated_text\n",
        "\n",
        "          chat_history.append((message, bot_message))\n",
        "\n",
        "          return chat_history\n",
        "\n",
        "        else:\n",
        "          bot_message = 'No Torah portion was selected to analyze!\\nPlease select Torah portion first!'\n",
        "\n",
        "          chat_history.append((message, bot_message))\n",
        "\n",
        "          return chat_history\n",
        "\n",
        "    prompts.input(fetch_torah_portion, [prompts], [imsg])\n",
        "    prompts1.input(fetch_torah_chapter, [prompts1], [imsg])\n",
        "    cmsg.input(respond, [cmsg, chatbot, imsg], [msg, chatbot])\n",
        "    msg.submit(respond, [msg, chatbot, imsg], [msg, chatbot])\n",
        "\n",
        "    aa_btn.click(auto_analysis, [chatbot, imsg], [chatbot])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "metadata": {
        "id": "prZ_9qiA81qD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Congrats! You did it! :)"
      ],
      "metadata": {
        "id": "LiZPP9BfTRtZ"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Torah Analyzer (ver. 1.0)\n",
        "\n",
        "***\n",
        "\n",
        "Powered by tegridy-tools: https://github.com/asigalov61/tegridy-tools\n",
        "\n",
        "***\n",
        "\n",
        "WARNING: This complete implementation is a functioning model of the Artificial Intelligence. Please excercise great humility, care, and respect. https://www.nscai.gov/\n",
        "\n",
        "***\n",
        "\n",
        "#### Project Los Angeles\n",
        "\n",
        "#### Tegridy Code 2024\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "cHXBchjTTcS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title NVIDA GPU Check\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "RAz7vARfQRO5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLU97viQMnXi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Install requirements\n",
        "!git clone https://github.com/asigalov61/tegridy-tools\n",
        "!pip install transformers -U\n",
        "!pip install accelerate -U\n",
        "!pip install gradio -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Import needed modules\n",
        "\n",
        "print('=' * 70)\n",
        "print('Loading modules...')\n",
        "print('=' * 70)\n",
        "\n",
        "%cd /content/tegridy-tools/tegridy-tools/\n",
        "\n",
        "import TORAH\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "print('=' * 70)\n",
        "print('Done!')\n",
        "print('=' * 70)"
      ],
      "metadata": {
        "id": "UFjK5rTjYvT8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load/Reload Mistral 7B Instruct model\n",
        "\n",
        "#@markdown https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map='cuda')"
      ],
      "metadata": {
        "id": "HLM3MAMgMv_O",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Start/Restart Torah Analyzer Gradio interface\n",
        "\n",
        "questions = [\n",
        "            'None',\n",
        "            'Please summarize the following text. Thank you.',\n",
        "            'Please write fifty questions about the provided text. Thank you.',\n",
        "            'Please write ten summary questions in order of importance. Thank you.',\n",
        "            'Please write twenty questions about the text in order of significance. Thank you.',\n",
        "            'Please write a short comprehensive essay about the most significant and important points of the text. Thank you.',\n",
        "            'Please write a short comprehensive literary summary essay about the most significant and important points of the text. Thank you.',\n",
        "            'Please write a short fictional essay based on the text. Thank you.',\n",
        "            'Please write a very long fictional essay based on the text. Thank you.',\n",
        "            'Please write a rephrased version of the provided text. Thank you.',\n",
        "            'Please write ten literary fictional titles for the provided text. Thank you.',\n",
        "            'Please write all possible questions about the provided text. Thank you.',\n",
        "            'Please write two hundred questions about the provided text. Thank you.',\n",
        "            'Please write ten most subtle, elusive, and tenious details from the provided text. Thank you.',\n",
        "            'Please write detailed statistical and numerical analysis of the provided text. Thank you.',\n",
        "            'Please write a single sentence summary of the provided text. Thank you.',\n",
        "            'Please write ten keywords of the text in order of importance and significance. Thank you.',\n",
        "            'What is the most important point of the text? Thank you.',\n",
        "            'What are the key numbers in the text? Thank you.',\n",
        "            'What are the key points of the text? Thank you.',\n",
        "            'What are the keywords of the text? Thank you.',\n",
        "            'What is the most significant point of the text?. Thank you.',\n",
        "            'What is the most intricate and delicate point in the provided text? Thank you.',\n",
        "\n",
        "            ]\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    def fetch_torah_portion(torah_portion):\n",
        "      if torah_portion != 'None':\n",
        "        tp = TORAH.torah_weekly_portion_getter(torah_portion_name=torah_portion)[2]\n",
        "        return tp\n",
        "\n",
        "      else:\n",
        "        return None\n",
        "\n",
        "    torah_portions = ['None'] + [t[0] for t in TORAH.TORAH_WEEKLY_PORTIONS]\n",
        "\n",
        "    gr.Markdown(\"<h1 style='text-align: center; margin-bottom: 1rem'>Torah Analyzer</h1>\")\n",
        "    prompts = gr.Dropdown(torah_portions, label=\"Select desired Torah portion to talk about\", value='None')\n",
        "    imsg = gr.Textbox(label='Selected Torah portion to analyze')\n",
        "    chatbot = gr.Chatbot(show_copy_button=True)\n",
        "    cmsg = gr.Dropdown(questions, label=\"Select a sample question or request to ask about\")\n",
        "    msg = gr.Textbox(label='Enter your own question or request to ask about the text')\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "    def respond(message, chat_history, imessage):\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        gen_len = 5000\n",
        "        num_batches = 1\n",
        "        temperature = 0.9\n",
        "\n",
        "        instruct_prompt = message\n",
        "\n",
        "        #========================================================================\n",
        "\n",
        "        if imessage and message != 'None':\n",
        "\n",
        "          prompt = '<s>[INST]' + instruct_prompt + '\\n' + imessage + '\\n' + instruct_prompt + '[/INST]' + '\\n'\n",
        "\n",
        "          input_ids = torch.LongTensor([tokenizer.encode(prompt)] * num_batches).cuda()\n",
        "\n",
        "          print('=' * 70)\n",
        "          print('Req len', len(input_ids[0]))\n",
        "          print('=' * 70)\n",
        "\n",
        "          output = model.generate(input_ids,\n",
        "                                  max_length=len(input_ids[0]) + gen_len,\n",
        "                                  temperature=temperature,\n",
        "                                  do_sample=True,\n",
        "                                  pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "          generated_ids = output[:, len(input_ids[0]):]\n",
        "\n",
        "          for g in generated_ids:\n",
        "            generated_text = tokenizer.decode(g, skip_special_tokens=True)\n",
        "\n",
        "          bot_message = generated_text\n",
        "\n",
        "          chat_history.append((message, bot_message))\n",
        "\n",
        "          return \"\", chat_history\n",
        "\n",
        "        else:\n",
        "          bot_message = 'No Torah portion was selected to analyze!\\nPlease select Torah portion first!'\n",
        "\n",
        "          chat_history.append((message, bot_message))\n",
        "\n",
        "          return \"\", chat_history\n",
        "\n",
        "    prompts.input(fetch_torah_portion, [prompts], [imsg])\n",
        "    cmsg.input(respond, [cmsg, chatbot, imsg], [msg, chatbot])\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot, imsg], [msg, chatbot])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "metadata": {
        "id": "prZ_9qiA81qD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Congrats! You did it! :)"
      ],
      "metadata": {
        "id": "LiZPP9BfTRtZ"
      }
    }
  ]
}
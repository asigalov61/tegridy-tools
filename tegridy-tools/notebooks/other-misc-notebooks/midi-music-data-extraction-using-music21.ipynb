{"cells":[{"metadata":{"_uuid":"2fd599f2037052b7ef1963a9a17f38066a01e491"},"cell_type":"markdown","source":"This is a small notebook regarding MIDI file manipulation using Music21, in this file we will:\n* Open MIDI files;\n* Plot MIDI music data;\n* Manipulate notes and chords;\n* Use Word2Vec to analyze chords.\n\n[You can find a high-level description of it on this Medium article.](https://medium.com/@wfaria_1/midi-music-data-extraction-using-music21-and-word2vec-on-kaggle-cb383261cd4e)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Listing current data on our folder.\nimport os\nprint(os.listdir(\".\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Opening MIDI files\n\nLets start creating a small method  to download MIDI files:"},{"metadata":{"trusted":true,"_uuid":"1a336908386684c050c608dd702cec92d0304755"},"cell_type":"code","source":"# Defining some constants and creating a new folder for MIDIs.\nmidi_path = \"MIDIs\"\nsonic_folder = \"sonic\"\n\n!rm -r $midi_path\n!mkdir $midi_path\n\n# Some helper methods.    \ndef concat_path(path, child):\n    return path + \"/\" + child\n\ndef download_midi(midi_url, path):\n    !wget $midi_url --directory-prefix $path > download_midi.log\n\n# Downloading an example file.\nsonic_path = concat_path(midi_path, sonic_folder)\ndownload_midi(\n    \"https://files.khinsider.com/midifiles/genesis/sonic-the-hedgehog/green-hill-zone.mid\",\n    sonic_path)\n    \nprint(os.listdir(sonic_path))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ddd5b77a220cf52d3aa1ee2a8987c571ae8f424"},"cell_type":"markdown","source":"Now we have a midi file there, lets try to open it using music21 library, which seems to be a robust platform to explore music files and music theory.\n\nNote that I included it as an external package on this Kaggle notebook. I used its [github link](https://github.com/cuthbertLab/music21) to do that. You can find its documentation [here](http://web.mit.edu/music21/doc/usersGuide/index.html)."},{"metadata":{"trusted":true,"_uuid":"b5bc6928da33cea8b4092e82df8fff9421c551ee"},"cell_type":"code","source":"from music21 import converter, corpus, instrument, midi, note, chord, pitch\n\ndef open_midi(midi_path, remove_drums):\n    # There is an one-line method to read MIDIs\n    # but to remove the drums we need to manipulate some\n    # low level MIDI events.\n    mf = midi.MidiFile()\n    mf.open(midi_path)\n    mf.read()\n    mf.close()\n    if (remove_drums):\n        for i in range(len(mf.tracks)):\n            mf.tracks[i].events = [ev for ev in mf.tracks[i].events if ev.channel != 10]          \n\n    return midi.translate.midiFileToStream(mf)\n    \nbase_midi = open_midi(concat_path(sonic_path, \"green-hill-zone.mid\"), True)\nbase_midi\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4922a83e8d9db763eb1444e88a065c1a07fbdad6"},"cell_type":"markdown","source":"## Debugging MIDI data\n\nWe can start listing the instruments used on that music:"},{"metadata":{"trusted":true,"_uuid":"64c036b215d5c178e8c01cb6d4fccb3f59d63bc1"},"cell_type":"code","source":"def list_instruments(midi):\n    partStream = midi.parts.stream()\n    print(\"List of instruments found on MIDI file:\")\n    for p in partStream:\n        aux = p\n        print (p.partName)\n\nlist_instruments(base_midi)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81c96306c7011a9b067f77bf44b762dd461d9b53"},"cell_type":"markdown","source":"Well we got a lot of unnamed instruments and one guitar... Differently from Scores created on Sheet editors like [MuseScore](https://musescore.org), which are crafted to be read by a musician. MIDIs are usually genereted on Digital Audio Workstations (DAW) as [LMMS](https://lmms.io) which have as objective the music audio generation. This way the presentation can be pretty unfriendly.\n\nCheck the following plot with notes as if we are using a DAW displaying the composition on the [piano roll](https://lmms.io/wiki/index.php?title=Piano_Roll_Editor):"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a1671c0fae0103df2b56faa99d264f752cee5756"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\n\ndef extract_notes(midi_part):\n    parent_element = []\n    ret = []\n    for nt in midi_part.flat.notes:        \n        if isinstance(nt, note.Note):\n            ret.append(max(0.0, nt.pitch.ps))\n            parent_element.append(nt)\n        elif isinstance(nt, chord.Chord):\n            for pitch in nt.pitches:\n                ret.append(max(0.0, pitch.ps))\n                parent_element.append(nt)\n    \n    return ret, parent_element\n\ndef print_parts_countour(midi):\n    fig = plt.figure(figsize=(12, 5))\n    ax = fig.add_subplot(1, 1, 1)\n    minPitch = pitch.Pitch('C10').ps\n    maxPitch = 0\n    xMax = 0\n    \n    # Drawing notes.\n    for i in range(len(midi.parts)):\n        top = midi.parts[i].flat.notes                  \n        y, parent_element = extract_notes(top)\n        if (len(y) < 1): continue\n            \n        x = [n.offset for n in parent_element]\n        ax.scatter(x, y, alpha=0.6, s=7)\n        \n        aux = min(y)\n        if (aux < minPitch): minPitch = aux\n            \n        aux = max(y)\n        if (aux > maxPitch): maxPitch = aux\n            \n        aux = max(x)\n        if (aux > xMax): xMax = aux\n    \n    for i in range(1, 10):\n        linePitch = pitch.Pitch('C{0}'.format(i)).ps\n        if (linePitch > minPitch and linePitch < maxPitch):\n            ax.add_line(mlines.Line2D([0, xMax], [linePitch, linePitch], color='red', alpha=0.1))            \n\n    plt.ylabel(\"Note index (each octave has 12 notes)\")\n    plt.xlabel(\"Number of quarter notes (beats)\")\n    plt.title('Voices motion approximation, each color is a different instrument, red lines show each octave')\n    plt.show()\n\n# Focusing only on 6 first measures to make it easier to understand.\nprint_parts_countour(base_midi.measures(0, 6))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a88c8f0fdb252ab31d4b5b751fe4d11a4528109"},"cell_type":"markdown","source":"If you know the [original song](https://www.youtube.com/watch?v=y-78CMKME4o), you can identify some known parts as the initial arpeggio at the beginning and the melody starting after it.  If we plot more measures, we can see the music structure. However, it is hard to extract pitch information from it.\n\nWe can take a look on the pitch histogram to see which notes are more used. If you know a bit of music theory, you'll notice that the seven more used notes are part of [C-major/A-minor Key](https://en.wikipedia.org/wiki/C_major), so this would be a good way to guess the music's key."},{"metadata":{"trusted":true,"_uuid":"3de39f70cf38449883bbb65aab010f72d4a1fe12"},"cell_type":"code","source":"base_midi.plot('histogram', 'pitchClass', 'count')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6e54caecd1921a69d5ee64574da3c2adf407683"},"cell_type":"markdown","source":"The scatter plot shows that the use of notes look consistent through time, so there are no key changes in this piece."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"24a6c8c9d4891f35f92b700fa29097651982af37"},"cell_type":"code","source":"base_midi.plot('scatter', 'offset', 'pitchClass')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"965d25cd8a05e6b9655ae351ba32a612412d68ea"},"cell_type":"markdown","source":"After seeing those graphics, we can be more confident in analyzing that music, lets take a look on some extra composition parameters:"},{"metadata":{"trusted":true,"_uuid":"e2a17ce5b60323a7244422ef86e822d125074fa9"},"cell_type":"code","source":"timeSignature = base_midi.getTimeSignatures()[0]\nmusic_analysis = base_midi.analyze('key')\nprint(\"Music time signature: {0}/{1}\".format(timeSignature.beatCount, timeSignature.denominator))\nprint(\"Expected music key: {0}\".format(music_analysis))\nprint(\"Music key confidence: {0}\".format(music_analysis.correlationCoefficient))\nprint(\"Other music key alternatives:\")\nfor analysis in music_analysis.alternateInterpretations:\n    if (analysis.correlationCoefficient > 0.5):\n        print(analysis)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ce452dc98230c828cd4a62732a37423442d0a10"},"cell_type":"markdown","source":"## Harmonic Reduction\n\nIt would be good to have the harmonic sequence of the music to analyze how it was built and to compare it with other musics. [Citing the wikipedia explanation about this](https://en.wikipedia.org/wiki/Reduction_(music%29):\n\n> In music, a reduction is an arrangement or transcription of an existing score or composition in which complexity is lessened to make analysis, performance, or practice easier or clearer; the number of parts may be reduced or rhythm may be simplified, such as through the use of block chords. \n\nLets start merging all voices on the same one and visualizing its contents:"},{"metadata":{"trusted":true,"_uuid":"f69f64685954f2174ef6848d4be7f29f27b6064d"},"cell_type":"code","source":"from music21 import stream\n\ntemp_midi_chords = open_midi(\n    concat_path(sonic_path, \"green-hill-zone.mid\"),\n    True).chordify()\ntemp_midi = stream.Score()\ntemp_midi.insert(0, temp_midi_chords)\n\n# Printing merged tracks.\nprint_parts_countour(temp_midi)\n\n# Dumping first measure notes\ntemp_midi_chords.measures(0, 1).show(\"text\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac5fa224a6e48a75a904692fa362e46d62f5e5fa"},"cell_type":"markdown","source":"The previous cell just merged all instruments, so each measure became flooded with a lot of chords. Each measure contains Four beats as showed previously, so lets try to display only the most used chord for each measure. We will try to find the chord counting the 4 most used notes per measure and trying to create a Chord with it. For example, if we find (E4, G4, C5, G5) then we would have a C-major chord on the [First inversion](https://en.wikipedia.org/wiki/Inversion_(music%29#Inversions):"},{"metadata":{"trusted":true,"_uuid":"4e28b031d92205e695a61650c8042aa441a7a8e2"},"cell_type":"code","source":"from music21 import roman\n\ndef note_count(measure, count_dict):\n    bass_note = None\n    for chord in measure.recurse().getElementsByClass('Chord'):\n        # All notes have the same length of its chord parent.\n        note_length = chord.quarterLength\n        for note in chord.pitches:          \n            # If note is \"C5\", note.name is \"C\". We use \"C5\"\n            # style to be able to detect more precise inversions.\n            note_name = str(note) \n            if (bass_note is None or bass_note.ps > note.ps):\n                bass_note = note\n                \n            if note_name in count_dict:\n                count_dict[note_name] += note_length\n            else:\n                count_dict[note_name] = note_length\n        \n    return bass_note\n                \ndef simplify_roman_name(roman_numeral):\n    # Chords can get nasty names as \"bII#86#6#5\",\n    # in this method we try to simplify names, even if it ends in\n    # a different chord to reduce the chord vocabulary and display\n    # chord function clearer.\n    ret = roman_numeral.romanNumeral\n    inversion_name = None\n    inversion = roman_numeral.inversion()\n    \n    # Checking valid inversions.\n    if ((roman_numeral.isTriad() and inversion < 3) or\n            (inversion < 4 and\n                 (roman_numeral.seventh is not None or roman_numeral.isSeventh()))):\n        inversion_name = roman_numeral.inversionName()\n        \n    if (inversion_name is not None):\n        ret = ret + str(inversion_name)\n        \n    elif (roman_numeral.isDominantSeventh()): ret = ret + \"M7\"\n    elif (roman_numeral.isDiminishedSeventh()): ret = ret + \"o7\"\n    return ret\n                \ndef harmonic_reduction(midi_file):\n    ret = []\n    temp_midi = stream.Score()\n    temp_midi_chords = midi_file.chordify()\n    temp_midi.insert(0, temp_midi_chords)    \n    music_key = temp_midi.analyze('key')\n    max_notes_per_chord = 4   \n    for m in temp_midi_chords.measures(0, None): # None = get all measures.\n        if (type(m) != stream.Measure):\n            continue\n        \n        # Here we count all notes length in each measure,\n        # get the most frequent ones and try to create a chord with them.\n        count_dict = dict()\n        bass_note = note_count(m, count_dict)\n        if (len(count_dict) < 1):\n            ret.append(\"-\") # Empty measure\n            continue\n        \n        sorted_items = sorted(count_dict.items(), key=lambda x:x[1])\n        sorted_notes = [item[0] for item in sorted_items[-max_notes_per_chord:]]\n        measure_chord = chord.Chord(sorted_notes)\n        \n        # Convert the chord to the functional roman representation\n        # to make its information independent of the music key.\n        roman_numeral = roman.romanNumeralFromChord(measure_chord, music_key)\n        ret.append(simplify_roman_name(roman_numeral))\n        \n    return ret\n\nharmonic_reduction(base_midi)[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87c3a0253ca56535047c0c18c3b9a3e42de30efa"},"cell_type":"markdown","source":"The previous harmonic reduction looks pretty confusing, maybe the melodic lines include several [nonchord notes](https://en.wikipedia.org/wiki/Nonchord_tone) which changes the Chord Quality. If we get a music like Bach's [Prelude in C-major](https://www.youtube.com/watch?v=RXeu8-j9k9Q) which uses a lot of chord arpeggios, we can see that our function works pretty well. At least on the beginning it gets almost all correct chords and other with minor errors."},{"metadata":{"trusted":true,"_uuid":"125acdba13a8516ee510400f3568e9e3f8cdfb4b"},"cell_type":"code","source":"from music21 import corpus\n\nbachChorale = corpus.parse('bach/bwv846')\nharmonic_reduction(bachChorale)[0:11]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c82f899afe18501c80cae4fa70ee52c93ca7ac29"},"cell_type":"markdown","source":"## MIDI Processing\n\nSo far we learned how to:\n* Open MIDI files;\n* Manipulate tracks and notes;\n* Plot music structure;\n* Analyze MIDI basic features as time signatures;\n* Analyze music elements as key signature and harmonic progressions.\n\nNow that we know the basics to handle MIDI data, lets download a bunch of MIDI files and play with them.\n\n"},{"metadata":{"trusted":true,"_uuid":"12eac76ca0ee5e55e3f4400c8130aee4ffa7282f"},"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n\ndef get_file_name(link):\n    filename = link.split('/')[::-1][0]\n    return filename\n\ndef download_file(link, filename):\n    mid_file_request = requests.get(link, stream=True)\n    if (mid_file_request.status_code != 200):\n        raise Exception(\"Failed to download {0}\".format(url))\n        \n    with open(filename, 'wb+') as saveMidFile:\n        saveMidFile.write(mid_file_request.content)\n\ndef download_midi_files(url, output_path):\n    site_request = requests.get(url)\n    if (site_request.status_code != 200):\n        raise Exception(\"Failed to access {0}\".format(url))\n    \n    soup = BeautifulSoup(site_request.content, 'html.parser')\n    link_urls = soup.find_all('a')\n\n    for link in link_urls:\n        href = link['href']\n        if (href.endswith(\".mid\")):\n            file_name = get_file_name(href)\n            download_path = concat_path(output_path, file_name)\n            midi_request = download_file(href, download_path)\n\ndef start_midis_download(folder, url):\n    !mkdir $folder # It is fine if this command fails when the directory already exists.\n    download_midi_files(url, folder)\n\ntarget_games = dict()\ntarget_games[\"sonic1\"] = \"https://www.khinsider.com/midi/genesis/sonic-the-hedgehog\"\ntarget_games[\"sonic2\"] = \"https://www.khinsider.com/midi/genesis/sonic-the-hedgehog-2\"\ntarget_games[\"sonic3\"] = \"https://www.khinsider.com/midi/genesis/sonic-the-hedgehog-3\"\ntarget_games[\"sonicAndKnuckles\"] = \"https://www.khinsider.com/midi/genesis/sonic-and-knuckles\"\n\nfor key, value in target_games.items():\n    file_path = concat_path(sonic_path, key)\n    start_midis_download(file_path, value)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"657e486eb86484f13da8cb787e5500733fc74978"},"cell_type":"markdown","source":"We downloaded more than 450 Sonic MIDIs (Almost 14MBs of files). The source site contains different versions of the same music created by fans, some of them are extended with new parts while others are more similar to the base composition, but they should share the same structure and key features. Lets finally convert them to a Pandas Dataframe including the harmonic reduction for each file."},{"metadata":{"trusted":true,"_uuid":"73c500dd12e792d6d72cd409f84e6227a33c8430"},"cell_type":"code","source":"# from multiprocessing.dummy import Pool as ThreadPool # Use this when IO is the problem\nfrom multiprocessing import Pool # Use this when CPU-intensive functions are the problem.\n\n# Go get a coffee, this cell takes hours to run...\ndef process_single_file(midi_param):\n    try:\n        game_name = midi_param[0]\n        midi_path = midi_param[1]\n        midi_name = get_file_name(midi_path)\n        midi = open_midi(midi_path, True)\n        return (\n            midi.analyze('key'),\n            game_name,\n            harmonic_reduction(midi),\n            midi_name)\n    except Exception as e:\n        print(\"Error on {0}\".format(midi_name))\n        print(e)\n        return None\n\ndef create_midi_dataframe(target_games):\n    key_signature_column = []\n    game_name_column = []\n    harmonic_reduction_column = []\n    midi_name_column = []\n    pool = Pool(8)\n    midi_params = []\n    for key, value in target_games.items():\n        folder_path = concat_path(sonic_path, key)\n        for midi_name in os.listdir(folder_path):\n            midi_params.append((key, concat_path(folder_path, midi_name)))\n\n    results = pool.map(process_single_file, midi_params)\n    for result in results:\n        if (result is None):\n            continue\n            \n        key_signature_column.append(result[0])\n        game_name_column.append(result[1])\n        harmonic_reduction_column.append(result[2])\n        midi_name_column.append(result[3])\n    \n    d = {'midi_name': midi_name_column,\n         'game_name': game_name_column,\n         'key_signature' : key_signature_column,\n         'harmonic_reduction': harmonic_reduction_column}\n    return pd.DataFrame(data=d)\n\nsonic_df = create_midi_dataframe(target_games)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e2a956768b0bf77fa71eca5336101320b64fe21"},"cell_type":"markdown","source":"There were some minor errors on the last cell but we were able to process almost all files. We can check the Key Signatures from those musics and notice that simpler Key Signatures as C-major, A-minor, F-major and D-minor were most common on the first Sonic games, then on Sonic 3 this changed (maybe because of [Michael Jackson](https://www.billboard.com/articles/news/6858197/michael-jackson-sonic-hedgehog-3-music-theory)? :P). Finally on Sonic and Knuckles it starts to use A-minor more frequently again."},{"metadata":{"trusted":true,"_uuid":"d80a3bcc446a11e88d39c24d0f2c26febff47b6b"},"cell_type":"code","source":"def key_hist(df, game_name, ax):\n    title = \"All Games Key Signatures\"\n    filtered_df = df\n    if (game_name is not None):\n        title = game_name + \" Key Signatures\"\n        filtered_df = df[df[\"game_name\"] == game_name]\n        \n    filtered_df[\"key_signature\"].value_counts().plot(ax = ax, kind='bar', title = title)\n\nfig, axes = plt.subplots(nrows=int(len(target_games)/3) + 1, ncols = 3, figsize=(12, 8))\nfig.subplots_adjust(hspace=0.4, wspace=0.3)\nkey_hist(sonic_df, None, axes[0, 0])\ni = 1\nfor key, value in target_games.items():\n    key_hist(sonic_df, key, axes[int(i/3), i%3])\n    i = i + 1\n\nsonic_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5702a8bebdb438bdd5a15243ae24958cad876613"},"cell_type":"markdown","source":"Since the harmonic sequences are just a list of strings, we could handle it as a common sentence from a text document. Lets try to use [word2vec](https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac) (more [here](https://medium.freecodecamp.org/how-to-get-started-with-word2vec-and-then-how-to-make-it-work-d0a2fca9dad3)) to find some relationships between chords on Sonic games compositions."},{"metadata":{"trusted":true,"_uuid":"4544dbbca594e1421f982a44d56eb953ca5fa8e6"},"cell_type":"code","source":"# import modules & set up logging\nimport gensim, logging\n# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n \nmodel = gensim.models.Word2Vec(sonic_df[\"harmonic_reduction\"], min_count=2, window=4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6281d515893cef1cfdb47037772e92de0f3c1e51"},"cell_type":"markdown","source":"For example, if we want to study how [Chord Substitutions](https://en.wikipedia.org/wiki/Chord_substitution) are applied on Sonic musics, we could find the most similar chords based on the model which we trained previously:"},{"metadata":{"_uuid":"526e2cee6b023ee227b623f99c25eaf1c6a945b4","trusted":true},"cell_type":"code","source":"def get_related_chords(token, topn=3):\n    print(\"Similar chords with \" + token)\n    for word, similarity in model.wv.most_similar(positive=[token], topn=topn):\n        print (word, round(similarity, 3))\n\ndef get_chord_similarity(chordA, chordB):\n    print(\"Similarity between {0} and {1}: {2}\".format(\n        chordA, chordB, model.wv.similarity(chordA, chordB)))\n    \nprint(\"List of chords found:\")\nprint(model.wv.vocab.keys())\nprint(\"Number of chords considered by model: {0}\".format(len(model.wv.vocab)))\n\nget_related_chords('I')\nget_related_chords('iv')\nget_related_chords('V')\n\n# The first one should be smaller since \"i\" and \"ii\" chord doesn't share notes,\n# different from \"IV\" and \"vi\" which share 2 notes.\nget_chord_similarity(\"I\", \"ii\") \nget_chord_similarity(\"IV\", \"vi\")\n\n# This one should be bigger because they are \"enharmonic\".\nget_chord_similarity(\"-i\", \"vii\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"107f033b8a8822f99f4ea38415f4f564f6897dbb"},"cell_type":"markdown","source":"One more example. We can try to calculate the similarity between musics using the harmonic reducton based on this [code](https://github.com/v1shwa/document-similarity/blob/master/DocSim.py). For example, if we would like to find more musics that looks like the Sonic 1's Green Hill theme, we could do this:"},{"metadata":{"trusted":true,"_uuid":"404d041cc5cb83e8b4a2a475ef5a6df7800e9148"},"cell_type":"code","source":"import pprint\ndef vectorize_harmony(model, harmonic_reduction):\n    # Gets the model vector values for each chord from the reduction.\n    word_vecs = []\n    for word in harmonic_reduction:\n        try:\n            vec = model[word]\n            word_vecs.append(vec)\n        except KeyError:\n            # Ignore, if the word doesn't exist in the vocabulary\n            pass\n    \n    # Assuming that document vector is the mean of all the word vectors.\n    return np.mean(word_vecs, axis=0)\n\ndef cosine_similarity(vecA, vecB):\n    # Find the similarity between two vectors based on the dot product.\n    csim = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))\n    if np.isnan(np.sum(csim)):\n        return 0\n    \n    return csim\n\ndef calculate_similarity_aux(df, model, source_name, target_names=[], threshold=0):\n    source_harmo = df[df[\"midi_name\"] == source_name][\"harmonic_reduction\"].values[0]\n    source_vec = vectorize_harmony(model, source_harmo)    \n    results = []\n    for name in target_names:\n        target_harmo = df[df[\"midi_name\"] == name][\"harmonic_reduction\"].values[0]\n        if (len(target_harmo) == 0):\n            continue\n            \n        target_vec = vectorize_harmony(model, target_harmo)       \n        sim_score = cosine_similarity(source_vec, target_vec)\n        if sim_score > threshold:\n            results.append({\n                'score' : sim_score,\n                'name' : name\n            })\n                \n    # Sort results by score in desc order\n    results.sort(key=lambda k : k['score'] , reverse=True)\n    return results\n\ndef calculate_similarity(df, model, source_name, target_prefix, threshold=0):\n    source_midi_names = df[df[\"midi_name\"] == source_name][\"midi_name\"].values\n    if (len(source_midi_names) == 0):\n        print(\"Invalid source name\")\n        return\n    \n    source_midi_name = source_midi_names[0]\n    \n    target_midi_names = df[df[\"midi_name\"].str.startswith(target_prefix)][\"midi_name\"].values  \n    if (len(target_midi_names) == 0):\n        print(\"Invalid target prefix\")\n        return\n    \n    return calculate_similarity_aux(df, model, source_midi_name, target_midi_names, threshold)\n\npp = pprint.PrettyPrinter(width=41, compact=True)\npp.pprint(calculate_similarity(sonic_df, model, \"green-hill-zone.mid\", \"green\")) # sonic1 x sonic1 music\npp.pprint(calculate_similarity(sonic_df, model, \"green-hill-zone.mid\", \"emerald\")) # sonic1 x sonic2 music\npp.pprint(calculate_similarity(sonic_df, model, \"green-hill-zone.mid\", \"hydro\")) # sonic1 x sonic3 music\npp.pprint(calculate_similarity(sonic_df, model, \"green-hill-zone.mid\", \"sando\")) # sonic1 x s&k music","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e970e9987fac74cec6e0becaabd9bd800072a07b"},"cell_type":"markdown","source":"It is good to see that the Green hill music is pretty similar with itself and with other versions based on the same piece. When we compare it with other musics, the similarity value begins to decrease. "},{"metadata":{"_uuid":"290cca7f553baa5da3ceb63ae780f8a716335071"},"cell_type":"markdown","source":"## Final considerations\n\nThis was just a demonstrative code with some ideas to work with MIDI data. I believe that there is space to improve the chord detection and the harmonic reduction. Also I am not sure if the amount of data used here are enough to make word2vec train a model fine.  If you find some bug or have some feedback please let me know!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}